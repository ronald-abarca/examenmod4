{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examen Practico 2 - Modulo 4: Analisis de Clusters Deportes de Fantasia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "## **Contexto** \n",
    "-------------------------------\n",
    "\n",
    "Los deportes de fantasía son plataformas de juegos en línea donde los participantes seleccionan y gestionan equipos virtuales de deportistas profesionales reales. Según el desempeño de los jugadores en el mundo real, a los jugadores se les asignan puntos en la plataforma de deportes de fantasía en cada partido. El objetivo es crear el mejor equipo posible con un presupuesto fijo para conseguir el máximo de puntos de fantasía, y los usuarios compiten entre sí durante una liga o temporada deportiva completa. Algunos de estos deportes de fantasía requieren inversiones financieras reales para participar, con posibilidades de ganar recompensas monetarias y entradas gratuitas para el día del partido de forma periódica.\n",
    "\n",
    "El mercado de los deportes de fantasía ha experimentado un enorme crecimiento en los últimos años, con una valoración de \\\\$18,6 mil millones en 2019. El segmento de fútbol lideró en términos de participación de mercado en 2019, con más de 8 millones de participantes en todo el mundo, y se espera que mantener su dominio durante los próximos años. La digitalización es uno de los principales factores que impulsan el crecimiento del mercado de los deportes de fantasía, ya que permite a los participantes la oportunidad de competir a nivel global y poner a prueba sus habilidades. Con un aumento en el uso de teléfonos inteligentes y la disponibilidad de aplicaciones de deportes de fantasía, se espera que este mercado sea testigo de un aumento mundial y alcance una valoración de \\\\$ 48,6 mil millones para 2027.\n",
    "\n",
    "\n",
    "----------------------------\n",
    "## **Objectivo**\n",
    "-----------------------------\n",
    "\n",
    "OnSports es una plataforma de deportes de fantasía que tiene ligas de fantasía para muchos deportes diferentes y ha sido testigo de un número creciente de participantes en todo el mundo durante los últimos 5 años. Para cada jugador, se establece un precio al principio y el precio sigue cambiando con el tiempo según el desempeño de los jugadores en el mundo real. Con la nueva temporada de la Premier League inglesa a punto de comenzar, han recopilado datos de la temporada pasada y quieren analizarlos para determinar el precio de cada jugador de cara al inicio de la nueva temporada. OnSports lo contrató como científico de datos y le pidió que realizara un análisis de conglomerados para identificar jugadores con diferentes potenciales de cada jugador en función del desempeño de la temporada anterior. Esto les ayudará a comprender los patrones en el desempeño de los jugadores y los retornos de fantasía y a decidir el precio exacto que se fijará para cada jugador para la próxima temporada de fútbol.\n",
    "\n",
    "\n",
    "--------------------------\n",
    "## **Diccionario de Datos**\n",
    "--------------------------\n",
    "\n",
    "- **Player_Name:** Nombre del jugador.\n",
    "- **Club:** Club en el que juega el jugador.\n",
    "- **Position:** Posición en la que juega el jugador.\n",
    "- **Goals_Scored:** Número de goles marcados por el jugador en la temporada anterior.\n",
    "- **Assists:** Número de pases realizados por el jugador que condujeron a goles en la temporada anterior.\n",
    "- **Total_Points:** Número total de puntos Fantasy anotados por el jugador en la temporada anterior.\n",
    "- **Minutes:** Número de minutos disputados por el jugador en la temporada anterior.\n",
    "- **Goals_Conceded:** Número de goles encajados por el jugador en la temporada anterior.\n",
    "- **Creativity:** Una puntuación, calculada utilizando una serie de estadísticas, que evalúa el rendimiento del jugador en términos de generar oportunidades de gol para otros jugadores.\n",
    "- **Influence:** Una puntuación, calculada utilizando una variedad de estadísticas, que evalúa el impacto de un jugador en un partido, teniendo en cuenta acciones que podrían afectar directa o indirectamente el resultado del partido.\n",
    "- **Threat:** Una puntuación, calculada utilizando una variedad de estadísticas, que mide a los jugadores que tienen más probabilidades de marcar goles.\n",
    "- **Bonus:** Total de puntos de bonificación recibidos. Los tres jugadores con mejor desempeño en cada partido reciben puntos de bonificación adicionales basados en una puntuación calculada utilizando una variedad de estadísticas. Se otorgan 3 puntos al jugador con mayor puntuación, 2 al segundo mejor y 1 al tercero.\n",
    "- **Clean_Sheets:** Número de partidos sin encajar gol en la temporada anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importar las librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='darkgrid')\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "# To scale the data using z-score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# To compute distances\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "# To perform K-means clustering and compute silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# To import K-Medoids\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# To import DBSCAN and Gaussian Mixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# To perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "\n",
    "# To suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's view a sample of the data\n",
    "data.sample(n = 10, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the column names and datatypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the data to another variable to avoid any changes to original data\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WapzauPrhmV7",
    "outputId": "011488d0-725d-420a-eee1-e6bed30758d8"
   },
   "outputs": [],
   "source": [
    "# Check for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhPuzWO7hmV8"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW4-5-GUhmV8"
   },
   "source": [
    "**Let's check the statistical summary of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(_____).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights: _____**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Univariate Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a boxplot and a histogram along the same scale\n",
    "\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize = (12, 7), kde = False, bins = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12, 7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    \n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows = 2,  # Number of rows of the subplot grid= 2\n",
    "        sharex = True,  # X-axis will be shared among all subplots\n",
    "        gridspec_kw = {\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize = figsize,\n",
    "    )  # Creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data = data, x = feature, ax = ax_box2, showmeans = True, color = \"violet\"\n",
    "    )  # Boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data = data, x = feature, kde = kde, ax = ax_hist2, bins = bins, palette = \"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data = data, x = feature, kde = kde, ax = ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color = \"green\", linestyle = \"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color = \"black\", linestyle = \"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Goals_Scored`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df, 'Goals_Scored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Assists`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(_______)  # Complete the code to create histogram_boxplot for 'Assists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Goals_Conceded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Clean_Sheets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Total_Points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Creativity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Influence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Threat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram and the boxplot for 'Bonus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights for all the plots: _____**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc = False, n = None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # Length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize = (count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize = (n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation = 90, fontsize = 15)\n",
    "    ax = sns.countplot(\n",
    "        data = data,\n",
    "        x = feature,\n",
    "        palette = \"Paired\",\n",
    "        order = data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # Percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # Count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # Width of the plot\n",
    "        y = p.get_height()  # Height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha = \"center\",\n",
    "            va = \"center\",\n",
    "            size = 12,\n",
    "            xytext = (0, 5),\n",
    "            textcoords = \"offset points\",\n",
    "        )  # Annotate the percentage\n",
    "\n",
    "    plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Club`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(df, 'Club')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Position`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot('_______')  # Complete the code to create a labelled barplot for 'Position'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:__**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga_huJrnhmWE"
   },
   "source": [
    "### **Bivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are done with univariate analysis. Let's explore the data a bit more with bivariate analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpJxawjAhmWE",
    "outputId": "78740200-cf4f-430e-d006-baf16bfb7a78"
   },
   "outputs": [],
   "source": [
    "# Correlation check\n",
    "cols_list = df.select_dtypes(include = np.number).columns.tolist()\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "sns.heatmap(\n",
    "    df[cols_list].corr(), annot = True, vmin = -1, vmax = 1, fmt = \".2f\", cmap = \"Spectral\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:__**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check players from which team have scored the most fantasy points on average.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 8))\n",
    "\n",
    "sns.barplot(data = df, x = ___ , y = ___ , ci = False)  # Complete the code to choose the right variables\n",
    "\n",
    "plt.xticks(rotation = 90) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We know that players in different positions have specific roles to play in a team. Let's check players in which positions tend to score more fantasy points on average.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code with the right variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To effectively utilize their squad depth, managers often rotate the squad to keep key players in shape for tougher games. Let's check the total number of minutes played, on average, across different positions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code with the right variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Every point counts in fantasy sports and getting bonus points for a player is always a treat. Let's check which team's players have secured the most bonus points, on average, last season.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code with the right variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see which players scored the most fantasy points last season for different positions of play.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = df.Position.unique().tolist()\n",
    "best_df = pd.DataFrame()\n",
    "\n",
    "for pos in pos_list:\n",
    "    df_aux = df[df.Position == pos]\n",
    "    best_df = best_df.append(df_aux[df_aux.Total_Points == df_aux.Total_Points.max()][['Player_Name', 'Club', 'Position', 'Total_Points']])\n",
    "\n",
    "best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see the top 10 players with the most fantasy points last season for different positions of play.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best10_df = pd.DataFrame()\n",
    "\n",
    "for pos in pos_list:\n",
    "    df_aux = df[df.Position == pos]\n",
    "    best10_df = best10_df.append(df_aux.sort_values('Total_Points', ascending = False).reset_index(drop = True).loc[:10, ['Player_Name', 'Club', 'Position', 'Total_Points']])\n",
    "\n",
    "best10_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outlier Check**\n",
    "\n",
    "- Let's plot the boxplots of all numerical columns to check for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "\n",
    "numeric_columns = df.select_dtypes(include = np.number).columns.tolist()\n",
    "\n",
    "for i, variable in enumerate(numeric_columns):\n",
    "    \n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    plt.boxplot(df[variable], whis = 1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaling**\n",
    "\n",
    "- Let's scale the data before we proceed with clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data before clustering\n",
    "scaler = ________ # Initialize the Standard Scaler\n",
    "\n",
    "subset = ___  # Complete the code to get the data with numerical features\n",
    "\n",
    "subset_scaled = ______ # Fit_transform the scaler function on data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the scaled data\n",
    "subset_scaled_df = pd.DataFrame(subset_scaled, columns = subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying PCA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of principal components to generate\n",
    "n = subset.shape[1]                                    # Storing the number of variables in the subset data\n",
    "\n",
    "pca = ____________                                     # Initialize PCA with n_components = n and random_state = 1\n",
    "\n",
    "data_pca = pd.DataFrame(pca.____________)              # Fit_transform PCA on the scaled data\n",
    "\n",
    "# The percentage of variance explained by each principal component is stored\n",
    "exp_var = pca.explained_variance_ratio_                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **K-Means Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_df = data_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = range(1, 15)\n",
    "meanDistortions = []\n",
    "\n",
    "for k in clusters:\n",
    "    \n",
    "    model = KMeans(n_clusters = k, random_state = 1)\n",
    "    \n",
    "    model.fit(data_pca)\n",
    "    \n",
    "    prediction = model.predict(k_means_df)\n",
    "    \n",
    "    distortion = (\n",
    "        sum(np.min(cdist(k_means_df, model.cluster_centers_, \"euclidean\"), axis = 1))\n",
    "        / k_means_df.shape[0]\n",
    "    )\n",
    "\n",
    "    meanDistortions.append(distortion)\n",
    "\n",
    "    print(\"Number of Clusters:\", k, \"\\tAverage Distortion:\", distortion)\n",
    "\n",
    "plt.plot(clusters, meanDistortions, \"bx-\")\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "\n",
    "plt.ylabel(\"Average Distortion\")\n",
    "\n",
    "plt.title(\"Selecting k with the Elbow Method\", fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:**\n",
    "\n",
    "- We will move ahead with k = 4. **What can be the reason for the same?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(______, random_state = 1) # Create K-Means with nclusters = 4\n",
    "\n",
    "kmeans.fit(k_means_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the original data\n",
    "df1 = df.copy()\n",
    "\n",
    "# Adding K-Means cluster labels to the K-Means dataframe\n",
    "k_means_df[\"KM_segments\"] = kmeans.labels_\n",
    "\n",
    "# Adding K-Means cluster labels to the original dataframe\n",
    "df1[\"KM_segments\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster_profile = df1.groupby( ____ ).mean()  # Complete the code to groupby the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the \"count_in_each_segment\" feature in K-Means cluster profile\n",
    "\n",
    "km_cluster_profile[\"count_in_each_segment\"] = (\n",
    "    df1.groupby( ______ )[\"Total_Points\"].count().values)  # Complete the code to groupby the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the maximum average value among all the clusters for each of the variables\n",
    "km_cluster_profile.style.highlight_max(color = \"lightgreen\", axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Complete the code to print the players in each cluster. Hint: Use the KM_segments feature\n",
    "\n",
    "for cl in df1[ ___ ].unique(): \n",
    "    print(\"In cluster {}, the following players are present:\".format(cl))\n",
    "    print(df1[df1[ ____ ] == cl][\"Player_Name\"].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby([\"KM_segments\", \"Position\"])['Player_Name'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize = (20, 20))\n",
    "counter = 0\n",
    "\n",
    "for ii in range(3):\n",
    "    for jj in range(4):\n",
    "        if counter < 10:\n",
    "            sns.boxplot(\n",
    "                ax = axes[ii][jj],\n",
    "                data = df1,\n",
    "                y = df1.columns[3 + counter],\n",
    "                x = \"KM_segments\",\n",
    "            )\n",
    "            counter = counter + 1\n",
    "\n",
    "fig.tight_layout(pad = 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characteristics of each cluster:___**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **K-Medoids Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmed_df = data_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmed = KMedoids(______, random_state = 1) # Create K-Medoids with nclusters = 4\n",
    "kmed.fit(kmed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the original data\n",
    "df2 = df.copy()\n",
    "\n",
    "# Add K-Medoids cluster labels to K-Medoids data\n",
    "k_med_df[\"KMed_segments\"] = ________\n",
    "\n",
    "# Add K-Medoids cluster labels to original data\n",
    "df2[\"KMed_segments\"] =  _________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the cluster-wise mean of all the variables. Hint: First group 'df2' by cluster labels column and then find mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"count_in_each_segment\" column in K-Medoids cluster profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the maximum average value among all the clusters for each of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to print the players in each cluster. Hint: Use the KMed_segments feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot for each of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characteristics of each cluster:___**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of cluster profiles from K-Means and K-Medoids:_______________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Hierarchical Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df = data_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of distance metrics\n",
    "distance_metrics = [\"euclidean\", \"chebyshev\", \"mahalanobis\", \"cityblock\"]\n",
    "\n",
    "# List of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"weighted\"]\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for dm in distance_metrics:\n",
    "    for lm in linkage_methods:\n",
    "        Z = linkage(hc_df, metric = dm, method = lm)\n",
    "        c, coph_dists = cophenet(Z, pdist(hc_df))\n",
    "        print(\n",
    "            \"Cophenetic correlation for {} distance and {} linkage is {}.\".format(\n",
    "                dm.capitalize(), lm, c\n",
    "            )\n",
    "        )\n",
    "        if high_cophenet_corr < c:\n",
    "            high_cophenet_corr = c\n",
    "            high_dm_lm[0] = dm\n",
    "            high_dm_lm[1] = lm\n",
    "            \n",
    "# Printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print('*'*100)\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.\".format(\n",
    "        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's explore different linkage methods with Euclidean distance only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for lm in linkage_methods:\n",
    "    Z = linkage(hc_df, metric = \"euclidean\", method = lm)\n",
    "    c, coph_dists = cophenet(Z, pdist(hc_df))\n",
    "    print(\"Cophenetic correlation for {} linkage is {}.\".format(lm, c))\n",
    "    if high_cophenet_corr < c:\n",
    "        high_cophenet_corr = c\n",
    "        high_dm_lm[0] = \"euclidean\"\n",
    "        high_dm_lm[1] = lm\n",
    "        \n",
    "# Printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print('*'*100)\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} linkage.\".format(\n",
    "        high_cophenet_corr, high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's view the dendrograms for the different linkage methods with Euclidean distance only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n",
    "\n",
    "# Lists to save results of cophenetic correlation calculation\n",
    "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
    "compare = []\n",
    "\n",
    "# To create a subplot image\n",
    "fig, axs = plt.subplots(len(linkage_methods), 1, figsize = (15, 30))\n",
    "\n",
    "# We will enumerate through the list of linkage methods above\n",
    "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    Z = linkage(hc_df, metric = \"euclidean\", method = method)\n",
    "\n",
    "    dendrogram(Z, ax = axs[i])\n",
    "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
    "\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(hc_df))\n",
    "    axs[i].annotate(\n",
    "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
    "        (0.80, 0.80),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )\n",
    "\n",
    "    compare.append([method, coph_corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print a dataframe to compare cophenetic correlations for different linkage methods\n",
    "df_cc = pd.DataFrame(compare, columns = compare_cols)\n",
    "df_cc = df_cc.sort_values(by = \"Cophenetic Coefficient\")\n",
    "df_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCmodel = AgglomerativeClustering(n_clusters = ___ , affinity = ___ , linkage = ___ )  # Complete the code to define the hierarchical clustering with average linkage\n",
    "HCmodel.fit(hc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the original data\n",
    "df3 = df.copy()\n",
    "\n",
    "# Adding hierarchical cluster labels to the Heirarhical and original dataframes\n",
    "hc_df[\"HC_segments\"] = _______________\n",
    "df3[\"HC_segments\"] = _______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the cluster-wise mean of all the variables. Hint: First group 'df3' by cluster labels column and then find mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"count_in_each_segment\" column in hierarchical cluster profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the maximum average value among all the clusters for each of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to print the players in each cluster. Hint: Use the HC_segments feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that most of the players have been grouped into one cluster, and there are two very sparse clusters. This clustering does not look good as the clusters do not have enough variability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us try using Ward linkage as it has more distinct and separated clusters (as seen from it's dendrogram before). 4 appears to be a good number of clusters from the dendrogram for Ward linkage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCmodel = AgglomerativeClustering(n_clusters = ___ , affinity = ___ , linkage = ___ )  # Complete the code to define the hierarchical clustering with Ward Linkage\n",
    "HCmodel.fit(hc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the original data\n",
    "df3 = df.copy()\n",
    "\n",
    "# Adding hierarchical cluster labels to the Heirarhical and original dataframes\n",
    "hc_df[\"HC_segments\"] = _______________\n",
    "df3[\"HC_segments\"] = _______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the cluster-wise mean of all the variables. Hint: First group 'df3' by cluster labels column and then find mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"count_in_each_segment\" column in hierarchical cluster profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the maximum average value among all the clusters for each of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to print the players in each cluster. Hint: Use the HC_segments feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot for each of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characteristics of each cluster:___**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of cluster profiles from Hierarchical and previous algorithms:___________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GMM clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_df = data_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(________, random_state = 1) # Initializing the Gaussian Mixture algorithm with n_components = 4\n",
    "\n",
    "gmm.fit(_____) # Fit the Gaussian Mixture algorithm on the gmm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the original data\n",
    "df4 = df.copy()\n",
    "\n",
    "# Adding gmm cluster labels to the GMM and original dataframes\n",
    "gmm_df[\"GMM_segments\"] = ____________\n",
    "df4[\"GMM_segments\"] = _______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the cluster-wise mean of all the variables. Hint: First group 'df4' by cluster labels column and then find mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"count_in_each_segment\" column in gmm cluster profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the maximum average value among all the clusters for each of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to print the players in each cluster. Hint: Use the GMM_segments feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot for each of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characteristics of each cluster:___**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of cluster profiles from GMM and previous algorithms:______________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DBSCAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is a very powerful algorithm for finding high-density clusters, but the problem is determining the best set of hyperparameters to use with it. It includes two hyperparameters, `eps`, and `min samples`.\n",
    "\n",
    "Since it is an unsupervised algorithm, you have no control over it, unlike a supervised learning algorithm, which allows you to test your algorithm on a validation set. The approach we can follow is basically trying out a bunch of different combinations of values and finding the silhouette score for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is the silhouette score?**\n",
    "\n",
    "Silhouette score is one of the methods for evaluating the quality of clusters created using clustering algorithms such as K-Means. The silhouette score is a measure of how similar an object is to its cluster (cohesion) compared to other clusters (separation). Silhouette score has a range of [-1, 1].\n",
    "\n",
    "* Silhouette coefficients near +1 indicate that the sample is far away from the neighboring clusters. \n",
    "* Silhouette score near -1 indicates that those samples might have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_df = data_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing lists\n",
    "eps_value = [2,3]                       # Taking random eps value\n",
    "min_sample_values = [6,20]              # Taking random min_sample value\n",
    "\n",
    "# Creating a dictionary for each of the values in eps_value with min_sample_values\n",
    "res = {eps_value[i]: min_sample_values for i in range(len(eps_value))}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the silhouette_score for each of the combination\n",
    "high_silhouette_avg = 0                                               # Assigning 0 to the high_silhouette_avg variable\n",
    "high_i_j = [0, 0]                                                     # Assigning 0's to the high_i_j list\n",
    "key = res.keys()                                                      # Assigning dictionary keys to a variable called key\n",
    "for i in key:\n",
    "    z = res[i]                                                        # Assigning dictionary values of each i to z\n",
    "    for j in z:\n",
    "        db = DBSCAN(eps = i, min_samples = j).fit(dbscan_df)          # Applying DBScan to each of the combinations in dictionary\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype = bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "        silhouette_avg = silhouette_score(dbscan_df, labels)           # Finding silhouette score \n",
    "        print( \n",
    "            \"For eps value =\" + str(i),\n",
    "            \"For min sample =\" + str(j),\n",
    "            \"The average silhoutte_score is :\",\n",
    "            silhouette_avg,                                            # Printing the silhouette score for each of the combinations\n",
    "        )\n",
    "        if high_silhouette_avg < silhouette_avg:                       # If the silhouette score is greater than 0 or the previous score, it will get appended to the high_silhouette_avg list with its combination of i and j              \n",
    "            high_i_j[0] = i\n",
    "            high_i_j[1] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the highest silhouette score\n",
    "print(\n",
    "    \"Highest_silhoutte_avg is {} for eps = {} and min sample = {}\".format(\n",
    "        high_silhouette_avg, high_i_j[0], high_i_j[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying DBSCAN with the hyperparmeter values that we got\n",
    "\n",
    "# Fit DBSCAN algorithm with the above hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the original data\n",
    "df5 = df.copy()\n",
    "\n",
    "# Add DBSCAN cluster labels to dbscan data\n",
    "dbscan_df[\"db_segments\"] = ________________\n",
    "\n",
    "# Add DBSCAN cluster labels to original data\n",
    "df5[\"db_segments\"] =  ___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the cluster-wise mean of all the variables. Hint: First group 'df5' by cluster labels column and then find mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"count_in_each_segment\" column in hierarchical cluster profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the maximum average value among all the clusters for each of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- Changing the eps and min sample values will result in different DBSCAN results? Can we try more value for eps and min_sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Choosing the Best Algorithm**\n",
    "\n",
    "- Since cluster profiles are the same for every algorithm except DBSCAN, it is difficult to choose the best algorithm. We can compute the silhouette score to choose the best algorithm among all the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4, random_state = 1)        # Initializing K-Means with number of clusters as 4 and random_state = 1\n",
    "\n",
    "preds = kmeans.fit_predict((data_pca))                   # Fitting and predicting K-Means on data_pca\n",
    "\n",
    "score = silhouette_score(data_pca, preds)                # Calculating the silhouette score\n",
    "\n",
    "print(score)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-Medoids with number of clusters as 4 and random_state = 1\n",
    "\n",
    "# Fitting and predicting K-Medoids on data_pca\n",
    "\n",
    "# Calculate the silhouette score\n",
    "\n",
    "# Print the score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Agglomerative Clustering with distance as Euclidean, linkage as ward with clusters = 4\n",
    "\n",
    "# Fitting and predicting HC algorithm on data_pca  \n",
    "\n",
    "# Calculate the silhouette score\n",
    "\n",
    "# Print the score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gaussian Mixture algorithm with number of clusters as 4 and random_state = 1\n",
    "\n",
    "# Fitting and predicting Gaussian Mixture algorithm on data_pca\n",
    "\n",
    "# Calculate the silhouette score\n",
    "\n",
    "# Print the score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- Which is the best algorithm here among all the algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxrEHj3ihmWO"
   },
   "source": [
    "## **Conclusion:**__\n",
    "\n",
    "\n",
    "## **Recommendations:**__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
